# ğŸ“š AI Personal Research Assistant (RAG + LLMs)

An end-to-end **Retrieval-Augmented Generation** app: upload PDFs (research papers, notes, docs), index them with **ChromaDB**, and ask questions via **OpenAI LLMs**. Answers are grounded in your documents and include **source citations**. Built with **LangChain** + **Streamlit**.



## âœ¨ Features

- **ğŸ“¥ PDF ingestion** â€” drag & drop one or many PDFs
- **ğŸ” Chunking + Embedding** â€” smart splits with `RecursiveCharacterTextSplitter`
- **ğŸ§  Vector store (persistent)** â€” `ChromaDB` stored in `./storage/chroma`
- **ğŸ’¬ Chat UI** â€” Streamlit interface with history
- **ğŸ“Œ Citations** â€” shows filenames + pages; expandable source snippets
- **ğŸ›ï¸ Controls** â€” change top-K, MMR, temperature, model names
- **â™»ï¸ Reset** â€” one click to wipe and rebuild the index



## ğŸ—ï¸ Architecture (RAG Flow)

1. **Load PDFs** â†’ `PyPDFLoader`
2. **Split** into overlapping chunks â†’ `RecursiveCharacterTextSplitter`
3. **Embed** chunks â†’ `OpenAIEmbeddings (text-embedding-3-small)`
4. **Store** vectors â†’ `Chroma` (persisted to disk)
5. **Retrieve** top-K chunks via similarity or **MMR** (diverse)
6. **Generate** final answer with **ChatOpenAI** using a grounded prompt
7. **Cite** sources â†’ `(source: filename p.<page>)`

```
PDFs â†’ Loader â†’ Splitter â†’ Embeddings â†’ Chroma (persist)
                                  â†‘                     â†“
                             Retriever  â†  Question  â†’ LLM (Answer + Citations)
```



## ğŸ§° Tech Stack

- **LangChain** (`langchain`, `langchain-community`, `langchain-openai`)
- **ChromaDB** for vector storage
- **OpenAI** GPT (default: `gpt-4o-mini`) and embeddings (`text-embedding-3-small`)
- **Streamlit** for the UI
- **pypdf** for robust PDF parsing



## ğŸš€ Quickstart

### 1) Clone & install
```bash
git clone https://github.com/yourname/ai-research-assistant.git
cd ai-research-assistant
python -m venv .venv && source .venv/bin/activate # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
```

### 2) Configure keys
```bash
cp .env.example .env
# Edit .env and set OPENAI_API_KEY
```

> You can also paste your key in the Streamlit sidebar at runtime.

### 3) Run the app
```bash
streamlit run streamlit_app.py
```

Open the displayed local URL in your browser.


## ğŸ–¥ï¸ Using the App

1. **Upload PDFs** under â€œğŸ“¤ Upload PDFsâ€
2. Click **ğŸ§  Build/Update Index** to embed and persist them (first time required)
3. Ask questions in **ğŸ’¬ Chat**  
4. Expand **Sources** to inspect the actual snippets and pages used

**Tips**  
- Use **specific** questions for better grounding  
- Increase **Top-K** to gather more evidence; enable **MMR** for diversity  
- If answers look off, **Reset Vector Store** and re-index



## ğŸ”§ Configuration

Edit `.env` or use sidebar inputs:

| Key | Purpose | Default |
|---|---|---|
| `OPENAI_API_KEY` | OpenAI access | _required_ |
| `OPENAI_MODEL` | Chat model | `gpt-4o-mini` |
| `OPENAI_EMBEDDING_MODEL` | Embeddings | `text-embedding-3-small` |
| `CHROMA_DIR` | Vector store path | `./storage/chroma` |
| `DOCS_DIR` | PDF storage | `./data/uploads` |
| `CHUNK_SIZE` | Chunk chars | `1000` |
| `CHUNK_OVERLAP` | Overlap chars | `200` |



## ğŸ§  How It Works (Deeper Dive)

### Chunking
We split docs with multiple separators (`\n\n`, `\n`, space, empty) to preserve semantics while filling chunks to ~1000 characters with **200 overlap**. This helps the retriever catch cross-sentence reasoning.

### Embeddings & Vector Store
Each chunk is embedded via `OpenAIEmbeddings` and stored in **Chroma** with metadata:
- `source`: original file path
- `page`: page number

### Retrieval
The retriever supports:
- **Similarity** search (`k` nearest)
- **MMR** (Maximal Marginal Relevance): balances relevance + diversity (great for long PDFs)

### Generation
A concise, **context-only** prompt is given to `ChatOpenAI`. If the fact isnâ€™t present in context, the model is instructed to say it doesnâ€™t know â€” reducing hallucination.



## ğŸ§ª Quality Tips

- **Multiple PDFs?** Index them together â€” RAG will pull the best chunks across files
- **Ask follow-ups** that reference previous answers; RAG remains grounded
- **Tune `k`**: try 4â€“8 for most research papers
- **Check sources**: if a claim looks surprising, expand the source snippet and verify



## ğŸ”’ Privacy & Localness

- Your PDFs are stored locally under `./data/uploads`  
- The vector store is persisted under `./storage/chroma`  
- Only **text embeddings & prompts** are sent to OpenAIâ€™s API

> Want fully local inference? You can swap `OpenAIEmbeddings` and `ChatOpenAI` for local providers (e.g., `HuggingFaceEmbeddings` + `Ollama`) with minor code changes.



## ğŸ› ï¸ Common Issues & Fixes

- **No API key / auth errors**  
  Set `OPENAI_API_KEY` in `.env` or paste in the sidebar.

- **â€œNo PDFs foundâ€ after ingest**  
  Ensure files are saved to `DOCS_DIR` (shown in sidebar). Re-click **Build/Update Index**.

- **Weird answers**  
  Increase **Top-K** and enable **MMR**. Re-ingest after adding more PDFs.

- **Version conflicts**  
  Use the exact `requirements.txt` and a fresh virtual environment.



## ğŸ§© Extending the Project

- **Add a FastAPI backend** for multi-user serving
- **Citations UI** with clickable anchors to exact pages
- **Multi-modal**: OCR scanned PDFs (e.g., `pytesseract`) + images
- **Summarization mode**: produce section-wise summaries per paper


